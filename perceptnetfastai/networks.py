# AUTOGENERATED! DO NOT EDIT! File to edit: ../Notebooks/01_Model/01_00_perceptnet.ipynb.

# %% auto 0
__all__ = ['GDN', 'PerceptNet', 'loss_perceptnet_fn']

# %% ../Notebooks/01_Model/01_00_perceptnet.ipynb 4
# from einops import rearrange, repeat, reduce
import einops as ein

import torch
import torch.nn as nn
import torch.nn.functional as F

# %% ../Notebooks/01_Model/01_00_perceptnet.ipynb 6
class GDN(nn.Module):
    """GDN custom layer."""
    
    def __init__(self,
                 in_channels, # Number of channels at input.
                 out_channels, # Number of channels at output.
                 kernel_size=3, # Kernel of the convolution.
                 gamma_init=.1, # Gamma parameter.
                 alpha_init=2, # Initial value of alpha.
                 epsilon_init=1/2, # Initial value of epsilon.
                 alpha_trainable=False, # Wether alpha is a trainable parameter.
                 epsilon_trainable=False, # Wether epsilon is a trainable parameter.
                 reparam_offset=2**(-18), # Numerical stability trick.
                 beta_min=1e-6, # Minimum value of beta.
                 apply_independently=False, # Wether to do grouped convolutions or not.
                 kernel_initializer="identity", # Initialization of the convolution kernel.
                 data_format="channels_first", # Format of the input data.
                 **kwargs):

        super(GDN, self).__init__()
        self.out_channels = out_channels
        self.kernel_size = kernel_size
        self.gamma_init = gamma_init
        self.reparam_offset = reparam_offset
        self.beta_min = beta_min
        self.beta_reparam = (self.beta_min+self.reparam_offset**2)**(1/2)
        self.apply_independently = apply_independently
        self.groups = in_channels if self.apply_independently else 1
        self.kernel_initializer = kernel_initializer
        self.data_format = data_format
        
        self.alpha_init = torch.tensor(alpha_init)
        self.epsilon_init = torch.tensor(epsilon_init)
        self.alpha_trainable = alpha_trainable
        self.epsilon_trainable = epsilon_trainable

        self.alpha = nn.Parameter(self.alpha_init, requires_grad=self.alpha_trainable)
        self.epsilon = nn.Parameter(self.epsilon_init, requires_grad=self.epsilon_trainable)
        self.init_params()

    def init_params(self):
        """Initializes the kernel and bias of the convolution."""
        gamma_bound = self.reparam_offset
        gamma = torch.ones((self.kernel_size, self.kernel_size))
        gamma = ein.repeat(gamma, "h w -> out_ch in_ch h w", out_ch=self.out_channels, in_ch=self.out_channels//self.groups)
        gamma = torch.sqrt(self.gamma_init*gamma + self.reparam_offset**2)
        gamma = gamma @ gamma
        self.gamma = nn.Parameter(gamma, requires_grad=True)
        beta = torch.ones(self.out_channels)
        beta = torch.sqrt(beta + self.reparam_offset**2)
        self.beta = nn.Parameter(beta, requires_grad=True)


    def forward(self, 
                X, # Input to the layer.
                ):
        """Perform a convolution with the input image and normalize by the result."""

        X_norm = torch.pow(X, self.alpha)
        X_norm = F.conv2d(X_norm, self.gamma, bias=self.beta, stride=1, padding="same", groups=self.groups)
        X_norm = torch.sqrt(X_norm)
        return X/X_norm

# %% ../Notebooks/01_Model/01_00_perceptnet.ipynb 11
class PerceptNet(nn.Module):
    """Basic PerceptNet architecture."""

    def __init__(self, 
                 in_channels, # Input channels.
                 ):
        super(PerceptNet, self).__init__()
        self.feature_extractor = nn.Sequential(GDN(in_channels=in_channels, out_channels=in_channels, kernel_size=1, apply_independently=True),
                                               nn.Conv2d(in_channels=in_channels, out_channels=3, kernel_size=1, padding="same"),
                                               nn.MaxPool2d(kernel_size=2),
                                               GDN(in_channels=3, out_channels=3, kernel_size=1),
                                               nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5, padding="same"),
                                               nn.MaxPool2d(kernel_size=2),
                                               GDN(in_channels=6, out_channels=6, kernel_size=1),
                                               nn.Conv2d(in_channels=6, out_channels=128, kernel_size=5, padding="same"),
                                               GDN(in_channels=128, out_channels=128, kernel_size=1))

    def forward(self,
                # X, # Input to the model.
                ref_img,
                dist_img,
                ):
        # ref_img, dist_img = X
        return self.feature_extractor(ref_img), self.feature_extractor(dist_img)

# %% ../Notebooks/01_Model/01_00_perceptnet.ipynb 13
def loss_perceptnet_fn(imgs, # Tuple of (ref_imgs, dist_imgs) in the transformed space.
                       mos, # Real Mean Opinion Score
                       ): # Loss
                       
    ## 1. Calculate the distance between the images
    ref_imgs, dist_imgs = imgs
    imgs_dist = (ref_imgs-dist_imgs)**2
    imgs_dist = ein.reduce(imgs_dist, "batch ch h w -> batch", "sum")
    imgs_dist = torch.sqrt(imgs_dist)

    ## 2. Correlate with the MOS (Copied from Alex)
    score_n = imgs_dist - torch.mean(imgs_dist)
    mos_n = mos - torch.mean(mos)
    score_n_norm = torch.sqrt(torch.mean(score_n**2))
    mos_n_norm = torch.sqrt(torch.mean(mos_n**2))
    denom = score_n_norm * mos_n_norm
    
    return -torch.mean(score_n*mos_n.squeeze(), dim=0) / denom
